---
title: "LAS stats extraction"
author: "Kristin Braziunas"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '../../')
```

## Purpose

The purpose of this script is to extract consistent LiDAR metrics from LAS data for use in fitting fuels regressions,
predicting fuel loads in Berry Fire sample points (Q1 and Q2), and creating the final fuels map. This script also creates
the common grid and masks used for final fuels map.

This script is not evaluated when knitted due to computation time.

```{r load-libs, results="hide", message=FALSE, warning=FALSE}
# load libraries
library(raster) # version 3.0-2
library(sp) # version 1.3-1
library(rgdal) # version 1.4-4
library(lidR) # version 2.1.4
library(dplyr) # version 0.8.3
library(sf) # version 0.7-7
library(fasterize) # version 1.0.0
library(rgeos) # version 0.5-1

###
# IMPORTANT: lidR version 2.1.4 is used here, this package has been updated since then
# and this code may not work in updated versions
###
```

## 1. Load data

This chunk loads a 1m DEM previously created from LiDAR data and the LiDAR data catalog. See comments
in chunk for how LAS points are classified.

```{r load-data, eval=FALSE}
### read in dem

# 1m dem  
grte.dem = raster("processed_data/GRTE_rasters/grte_1m_dem_nad2011.tif")

### las file

# classification
# 1: default/unclassified = EXCLUDE
# 2: ground = EXCLUDE from stats, INCLUDE to get ground elevation
# 3: low vegetation 0.1 to 0.5 m in height = INCLUDE
# 4: medium vegetation 0.5 to 2 m in height = INCLUDE
# 5: high vegetation 2 m to 60 m in height = INCLUDE
# 6: buildings = EXCLUDE
# 7: noise = EXCLUDE
# 9: water = EXCLUDE
# 10: ignored ground = EXCLUDE
# 17: overlap default = EXCLUDE
# 18: overlap ground = EXCLUDE

### catalog for full lidar dataste

# create catalog
grte.ctg = readLAScatalog("D:/future_projects/Modified_GRTE_LiDAR/LAS_1.2/")
grte.ctg@proj4string = CRS("+init=epsg:6341")  # crs is NAD83(2011) UTM12N, EPSG 6341
opt_filter(grte.ctg) = "-keep_class 2 3 4 5"  # add filter
```

## 2. Create function to extract LAS metrics

I created a function that could be applied to a set of plot shapefiles to extract associated
LiDAR predictor metrics.

```{r extract-function, eval=FALSE}
extract_metrics = function(shp.in) {
  
  # output dataframe
  lidar.metrics = data.frame()
  
  for(i in 1:length(shp.in)) {
    
    # subset to one point
    poly.in = shp.in[i,]
    
    plot(grte.ctg)
    plot(poly.in, col="red", add=TRUE)
    
    # clip to plot footprint
    grte.clip = lasclip(las=grte.ctg, geometry=poly.in)
    
    # can plot to check
    # plot(grte.clip, color="Classification")
    
    ground = lasfilter(grte.clip, Classification==2)
    veg = lasfilter(grte.clip, Classification %in% c(3,4,5))
    
    first.can = lasfilter(grte.clip, Classification %in% c(5) & ReturnNumber==1)
    first.shr = lasfilter(grte.clip, Classification %in% c(3,4) & ReturnNumber==1)
    first.all = lasfilter(grte.clip, Classification %in% c(2,3,4,5) & ReturnNumber==1)
    
    # could create dtm on the fly
    # dtm = grid_terrain(ground, algorithm=tin())
    
    # use 1-m dem created from DEM tiles generated by Woolpert
    dtm = grte.dem
    
    # # fix problems with na values
    # if(length(dtm[is.na(values(dtm))])>0) {
    #   print("yes")
    # }
    
    # normalize to dem
    veg.norm = lasnormalize(veg, dtm,na.rm=TRUE)
    
    # calculate metrics and save to dataframe
    zmax = lasmetrics(veg.norm, max(Z))
    zmean = lasmetrics(veg.norm, mean(Z))
    zcv = lasmetrics(veg.norm, cv(Z))
    zquants = lasmetrics(veg.norm, quantile(Z, probs=c(0.1,0.25,0.5,0.75,0.9,0.99)))
    names(zquants) = c("zp10","zp25","zp50","zp75","zp90","zp99")
    
    zfcc = (first.can@header@PHB$`Number of point records`/first.all@header@PHB$`Number of point records`) * 100
    zfsc = (first.shr@header@PHB$`Number of point records`/first.all@header@PHB$`Number of point records`) * 100
    
    z.out = data.frame(cbind(Plot_code=as.character(poly.in@data$Plot_code), zmax,zmean,zcv,t(zquants), zfcc,zfsc))
    
    lidar.metrics=rbind(lidar.metrics,z.out)
    
  }
  
  return(lidar.metrics)
}
```

## 3. Extract LiDAR metrics from 2019 field data

I used shapefiles for 2019 field data to extract LiDAR metrics for each circular plot. For one plot in which
trees were sampled in 1/4 of the plot, I used the shapefile for just that quadrant.

```{r field-data-metrics, eval=FALSE}
# load polygons
field_polys = readOGR("processed_data/field_plot_selection/final_plots/field_plots_polygons_2019.shp")
# load plot in which only 1/4 footprint was measured separately
berryglade_q1 = readOGR("processed_data/field_plot_selection/final_plots/berryglade_q1_polygon.shp")
berryglade_q1@data$Plot_code = "Con_BerryGlade_Unburned3"

# get in same crs
field.nad2011 = spTransform(field_polys, CRSobj=crs(grte.ctg))
berryglade.nad2011 = spTransform(berryglade_q1, CRSobj=crs(grte.ctg))

# run function
lidar.met = extract_metrics(field.nad2011)
lidar.met2 = extract_metrics(berryglade.nad2011)

# combine and replace BerryGlade_Unburned with appropriate plot footprint
lidar.all = lidar.met %>%
  filter(Plot_code != "Con_BerryGlade_Unburned3") %>%
  rbind(lidar.met2)

# write metrics as csv
write.csv(lidar.all, file="processed_data/fuels_map_variables/lidar_metrics.csv", row.names=FALSE)
```

## 4. Extract LiDAR metrics from Berry Fire random samples (Q1, Q2)

I then used randomly selected grid cells from within the Berry Fire (see step 03 script) to extract
the same LiDAR metrics for these plot fooprints (corresponding with grid cells in Berry Fire severity raster).

```{r berry-fire-metrics, eval=FALSE}

### load sampled polys
berry.age = readOGR("processed_data/Berry_Fire_sample_polys/berry_fire_age_sample.shp")
berry.wx = readOGR("processed_data/Berry_Fire_sample_polys/berry_fire_wx_sample.shp")

# get in correct crs
age.nad2011 = spTransform(berry.age, CRSobj=crs(grte.ctg))
wx.nad2011 = spTransform(berry.wx, CRSobj=crs(grte.ctg))

### start with age (q1)
# output dataframe
age.metrics = data.frame()
  
# age metrics
for(i in 1:length(age.nad2011)) {
  
  berry.poly = age.nad2011[i,]

  z.mets = extract_metrics(berry.poly)
  
  z.out = data.frame(cbind(sample_id = berry.poly@data$sample_id, 
                           x_ctr = berry.poly@polygons[[1]]@Polygons[[1]]@labpt[1], 
                           y_ctr = berry.poly@polygons[[1]]@Polygons[[1]]@labpt[2],
                           age = berry.poly@data$age,
                           rdnbr = berry.poly@data$rdnbr,
                           elev_m = berry.poly@data$elev_m,
                           slope_deg = berry.poly@data$slope_deg,
                           aspect_deg = berry.poly@data$aspect_deg,
                           z.mets))
  
  age.metrics=rbind(age.metrics,z.out)
  
}

write.csv(age.metrics, file="processed_data/Berry_Fire_sample_polys/berry_fire_age_lidar.csv", row.names=FALSE)

### thex wx (q2)
# output dataframe
wx.metrics = data.frame()

# wx metrics
for(i in 1:length(wx.nad2011)) {
  
  berry.poly = wx.nad2011[i,]
  
  z.mets = extract_metrics(berry.poly)
  
  z.out = data.frame(cbind(sample_id = berry.poly@data$sample_id, 
                           x_ctr = berry.poly@polygons[[1]]@Polygons[[1]]@labpt[1], 
                           y_ctr = berry.poly@polygons[[1]]@Polygons[[1]]@labpt[2],
                           weather = berry.poly@data$weather,
                           prev_fire = berry.poly@data$prev_fire,
                           date = berry.poly@data$date,
                           rdnbr = berry.poly@data$rdnbr,
                           elev_m = berry.poly@data$elev_m,
                           slope_deg = berry.poly@data$slope_deg,
                           aspect_deg = berry.poly@data$aspect_deg,
                           z.mets))
  
  wx.metrics=rbind(wx.metrics,z.out)
  
}

write.csv(wx.metrics, file="processed_data/Berry_Fire_sample_polys/berry_fire_wx_lidar.csv", row.names=FALSE)

```

## 5. Create common grid and masks

Prior to creating the fuels map, I created a common grid using a LANDFIRE raster for extracting
values from both LiDAR and imagery (see step 05 script). In addition, I used this common grid to create a series
of mask rasters for later use in creating the fuels map. These included separate masks for
all vegetation, conifer forest, deciduous forest, and shrubland. There is also a final
mask that excludes areas disturbed between remote sensing acquisition and field data
collection (2014-2019). These masks are all saved in `processed_data/GRTE_rasters/`.

```{r common-grid-masks, eval=FALSE}
### load, crop, and prep landfire master ID raster
# load in landfire raster to serve as common grid
lf.in = raster("data/LANDFIRE/US_200EVT/us_200evt/hdr.adf")

# use lidar catalog to create boundary
grte.shp = as.spatial(grte.ctg) # generate sp object

# change to sf object, dissolve, and transform
grte.nad2011 = st_as_sf(grte.shp) %>%
  mutate(id=1) %>%
  group_by(id) %>%
  summarise() %>%
  # add a little buffer
  st_buffer(30) %>%
  st_transform(crs=st_crs(grte.ctg))

# save this shapefile
st_write(grte.nad2011, "processed_data/GRTE_shps/lidar_footprint_nad2011.shp")

# use this shapefile to crop landfire
# transform
grte.aea = st_transform(grte.nad2011, crs=st_crs(lf.in))

plot(grte.aea) # looks good

# double check
plot(lf.in)
plot(grte.aea, add=TRUE)

# crop landfire with this shapfile
lf.crop = crop(lf.in, grte.aea, snap="out")
plot(lf.crop)
plot(grte.aea, add=TRUE)

# will need to have a raster in NAD2011 to extract lidar metrics
# using raster rather than iterating through multiple shapefiles
# is much more efficient, although it will mean there may be some offset
# between landfire and lidar fuels map

lf.nad2011 = projectRaster(lf.crop, res=30, crs=crs(grte.ctg))
plot(lf.nad2011)
plot(grte.ctg, add=TRUE)

# assign unique ID to each value in this raster
val.ids = 1:ncell(lf.nad2011)
lf.id = setValues(lf.nad2011, val.ids)
plot(lf.id)  # looks good
names(lf.id) = "unique_ids" # update name

# save this raster as master version
writeRaster(lf.id, "processed_data/GRTE_rasters/landfire_id_raster_nad2011.tif", format="GTiff")
# lf.id = raster("processed_data/GRTE_rasters/landfire_id_raster_nad2011.tif")

# some cleanup 
rm(lf.in, lf.crop, lf.nad2011, grte.shp)

### mask just to areas where will be mapping fuels, starting with outline and veg types

# do a bunch of masking
lf.mask = mask(lf.id,st_transform(grte.aea, st_crs(grte.ctg))) # mask by lidar footprint, looks good
# writeRaster(lf.mask, "processed_data/GRTE_rasters/landfire_id_raster_mask_nad2011.tif", format="GTiff", overwrite=TRUE)
# lf.mask = raster("processed_data/GRTE_rasters/landfire_id_raster_mask_nad2011.tif")
crs(lf.mask) = "+init=epsg:6341"

# read in vegetation map, only conifer, deciduous, and shrubland
grte.veg = st_read("processed_data/GRTE_Veg/GRTE_veg_dissolve.shp") %>%
  st_transform(crs(lf.mask))

# grte.veg # looks good
# plot(grte.veg) # looks good
# plot(lf.mask)
# plot(grte.veg,add=TRUE)

# important that each cell in the grid has a veg type assigned
# so next step is to rasterize this vegetation map

# first create a finer resolution version of the landfire raster, factor of 10
lf.fine = disaggregate(lf.mask, fact=10)
lf.fine

# create 4 versions - 1 that assigns value based on all veg attributes plus 1 each for conifer, deciduous, and shrubland
# create at 3m resolution
veg.all = fasterize(grte.veg, lf.fine, field="ECOLOGY", fun="any", background=0) %>%
  # aggregate back to 30m resolution, mean value gives proportion of that cell that is vegetated
  aggregate(fact=10, fun="mean")

grte.veg$ECOLOGY

# do this for each veg type
veg.con = grte.veg %>%
  filter(ECOLOGY %in% c("Coniferous Woodland")) %>%
  fasterize(lf.fine, field="ECOLOGY", fun="any", background=0) %>%
  aggregate(fact=10, fun="mean", na.rm=FALSE)

veg.dec = grte.veg %>%
  filter(ECOLOGY %in% c("Deciduous Forest")) %>%
  fasterize(lf.fine, field="ECOLOGY", fun="any", background=0) %>%
  aggregate(fact=10, fun="mean", na.rm=FALSE)

veg.shr = grte.veg %>%
  filter(ECOLOGY %in% c("Dwarf Shrubland","Upland Shrubland")) %>%
  fasterize(lf.fine, field="ECOLOGY", fun="any", background=0) %>%
  aggregate(fact=10, fun="mean", na.rm=FALSE)

# save all 4 of these
writeRaster(veg.all, "processed_data/GRTE_rasters/grte_30m_veg_all_nad2011.tif", format="GTiff")
writeRaster(veg.con, "processed_data/GRTE_rasters/grte_30m_veg_con_nad2011.tif", format="GTiff")
writeRaster(veg.dec, "processed_data/GRTE_rasters/grte_30m_veg_dec_nad2011.tif", format="GTiff")
writeRaster(veg.shr, "processed_data/GRTE_rasters/grte_30m_veg_shr_nad2011.tif", format="GTiff")

# # read in if needed
# veg.all = raster("processed_data/GRTE_rasters/grte_30m_veg_all_nad2011.tif")
# crs(veg.all) = "+init=epsg:6341"
# veg.con = raster("processed_data/GRTE_rasters/grte_30m_veg_con_nad2011.tif")
# crs(veg.con) = "+init=epsg:6341"
# veg.dec = raster("processed_data/GRTE_rasters/grte_30m_veg_dec_nad2011.tif")
# crs(veg.dec) = "+init=epsg:6341"
# veg.shr = raster("processed_data/GRTE_rasters/grte_30m_veg_shr_nad2011.tif")
# crs(veg.shr) = "+init=epsg:6341"

# now mask lf.mask by vegetation types of interest
# only include pixels that are at least 50% covered by vegetation types of interest
# create raster versions with binary coding based on cutoff
veg_bin = function(r.in, cutoff) {
  r.in[r.in >= cutoff] = 1
  r.in[r.in < cutoff] = NA
  return(r.in)
}

veg.mask = veg_bin(veg.all, 0.5)

lf.veg = mask(lf.mask, veg.mask)

# how many cells now
length(lf.veg[!is.na(lf.veg)]) # down to 983,000!!

### read in shapefiles for disturbance and management

# start with aerial IDS data
dgdb = "data/IDS/Region4_AllYears.gdb/Region4_AllYears.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
ogrListLayers(dgdb)

# Read the feature class
disturb.in = st_read(dsn=dgdb,layer="DAMAGE_AREAS_FLAT_AllYears_Rgn4") %>%
  # filter for only data 2013 and later
  filter(SURVEY_YEAR > 2013) %>%
  st_cast("MULTIPOLYGON") # to prevent downstream errors

disturb = disturb.in %>%
  # crop based on grte bounding box
  st_crop(st_bbox(st_transform(grte.nad2011, st_crs(disturb.in)))) %>%
  # transform to proper crs
  st_transform(crs(lf.veg))


# next read in fire perimeters
fgdb = "data/GRTE_FireHistory.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
ogrListLayers(fgdb)

# Read the feature class
fires.in = st_read(dsn=fgdb,layer="WF_Perimeters_flat_ply") %>%
  # filter for only data 2013 and later
  filter(FireCalendarYear > 2013) %>%
  st_cast("MULTIPOLYGON") # to prevent downstream errors

fires = fires.in %>%
  # crop based on grte bounding box
  st_crop(st_bbox(st_transform(grte.nad2011, st_crs(fires.in)))) %>%
  # transform to proper crs
  st_transform(crs(lf.veg))

# also read in fuels treatments feature class
treat1.in = st_read(dsn=fgdb,layer="FuelsTreatments_flat_ply") %>%
  # filter for only data 2013 and later
  filter(as.Date(ActualCompletionDate) > as.Date('2013-12-30 00:00:00')) %>%
  st_cast()

treat1 = treat1.in %>%
  # crop based on grte bounding box
  st_crop(st_bbox(st_transform(grte.nad2011, st_crs(treat1.in)))) %>%
  # transform to proper crs
  st_transform(crs(lf.veg))

# finally read in more recent treatments
treat2.in = st_read("data/GRTE_Shapefiles_Post2016/Fuels_Treatments_2015_2018.shp")

treat2 = treat2.in %>%
  # crop based on grte bounding box
  st_crop(st_bbox(st_transform(grte.nad2011, st_crs(treat2.in)))) %>%
  # transform to proper crs
  st_transform(crs(lf.veg))

# check
plot(lf.veg)
plot(disturb, add=TRUE)
plot(fires, add=TRUE)
plot(treat1, add=TRUE)
plot(treat2, add=TRUE)

# mask lf.veg by everything
lf.final = lf.veg %>%
  mask(disturb, inverse=TRUE) %>%
  mask(fires, inverse=TRUE) %>%
  mask(treat1, inverse=TRUE) %>%
  mask(treat2, inverse=TRUE)

plot(lf.final)
# how many cells now
length(lf.final[!is.na(lf.final)]) # down to 900,000!

# save this raster 
writeRaster(lf.final, "processed_data/GRTE_rasters/grte_30m_final_mask_nad2011.tif",format="GTiff")

# clean up before extracting lidar stats
rm(disturb,disturb.in,fires,fires.in,grte.veg,lf.fine,lf.id,lf.mask,lf.veg,treat1,treat1.in,treat2,treat2.in,veg.all,veg.con,veg.dec,veg.mask,veg.shr,dgdb,fgdb)

```

## 6. Extract LiDAR metrics for fuels map

For the fuels map, the LANDFIRE common grid is used to extract the same predictor variables
described above at 30m resolution for creating the final fuels map (covers GRTE, JODR, and the National Elk Refuge).
Although a masked version of the common grid raster is
used, the lidR package by default ignores masking and just uses the grid to extract values from
every grid cell within the LiDAR footprint. The only grid cells with no values are ones with
no points classified as ground or vegetation (i.e., outside of LiDAR footprint or water). The output
of running the extraction script on the full LiDAR dataset is a 30-m resolution raster for
each LiDAR tile. These tiles are later mosaiced into a 30m raster covering the entire fuels map footprint (see step 07 script).

```{r fuels-map-metrics, eval=FALSE}

### read in lf final
lf.final = raster("processed_data/GRTE_rasters/grte_30m_final_mask_nad2011.tif")
crs(lf.final) = crs(grte.ctg)

### prepare lidar for raster extraction

# use built in grid metrics for most stats
grte.metrics = function(z) {
  metrics = list(
    zmax = as.numeric(max(z)),
    zmean = as.numeric(mean(z)),
    zcv = as.numeric(cv(z)),
    zp10 = as.numeric(quantile(z, probs=0.1)),
    zp25 = as.numeric(quantile(z, probs=0.25)),
    zp50 = as.numeric(quantile(z, probs=0.5)),
    zp75 = as.numeric(quantile(z, probs=0.75)),
    zp90 = as.numeric(quantile(z, probs=0.9)),
    zp99 = as.numeric(quantile(z, probs=0.99))
  )
}

# ### use to test run with one las file
# las.in = readLAS("D:/future_projects/Modified_GRTE_LiDAR/LAS_1.2/12TWP235765.las", filter = "-keep_class 2 3 4 5" )
# las.in@proj4string = CRS("+init=epsg:6341")  # crs is NAD83(2011) UTM12N, EPSG 6341

stat.function <- function(chunk)
{
  # read las chunk
  las.in = readLAS(chunk, filter = "-keep_class 2 3 4 5")
  las.in@proj4string = CRS("+init=epsg:6341")
  if (is.empty(las.in)) return(NULL)
  
  # filter to veg points only
  veg = lasfilter(las.in, Classification %in% c(3,4,5))
  
  # normalize
  las.norm = lasnormalize(veg, grte.dem,na.rm=TRUE)
  
  # calculate custom metrics
  metrics = grid_metrics(las.norm, ~grte.metrics(Z), lf.final) %>%
    crop(raster::extent(las.in))
  
  first.can = grid_metrics(las.in, ~length(Z), lf.final, filter = ~ReturnNumber == 1 & Classification %in% c(5)) %>%
    raster::crop(raster::extent(las.in))
  first.shr = grid_metrics(las.in, ~length(Z), lf.final, filter = ~ReturnNumber == 1 & Classification %in% c(3,4)) %>%
    raster::crop(raster::extent(las.in))
  first.all = grid_metrics(las.in, ~length(Z), lf.final, filter = ~ReturnNumber == 1 & Classification %in% c(2,3,4,5)) %>%
    raster::crop(raster::extent(las.in))

  zfcc = (first.can/first.all)*100
  names(zfcc) = "zfcc"
  zfsc = (first.shr/first.all)*100
  names(zfsc) = "zfsc"
  
  metrics.out = stack(metrics,zfcc,zfsc)
  return(metrics.out)
}


### run on full lidar catalog
opt_output_files(grte.ctg) <- "processed_data/GRTE_rasters/lidar_tiles_new/lidar_tile_{XLEFT}_{YBOTTOM}"
metrics.full = catalog_apply(grte.ctg, stat.function)

# some errors, probably with the lake
# try labeling as processed and start again, omitting tiles that throw errors
metrics.full2 = catalog_apply(grte.ctg[474:746,], stat.function)
```